import axios from 'axios';
import envConfig from '../envConfig';
import { getAuthHeaders } from '../utils/authUtils';

/**
 * Document Service for AI Workflow Builder
 * Integrates with existing document system and blob storage
 * Now includes intelligent content extraction and chunking via backend processing
 */
class WorkflowDocumentService {
  constructor() {
    this.apiUrl = envConfig.apiUrl;
    this.blobStorageUrl = process.env.REACT_APP_AZURE_STORAGE_URL || 
                          'https://smartdocaistorage.blob.core.windows.net';
  }

  /**
   * Fetch all available documents from the system
   * @param {boolean} forceRefresh - Force refresh by adding cache buster
   */
  async fetchDocuments(forceRefresh = false) {
    try {
      const authHeaders = await getAuthHeaders();
      console.log('ðŸ” Fetching documents from:', `${this.apiUrl}/documents`);
      console.log('ðŸ”‘ Auth headers:', {
        hasAuth: !!authHeaders.Authorization,
        authType: authHeaders.Authorization ? authHeaders.Authorization.substring(0, 20) + '...' : 'None'
      });
      
      // Check if user is authenticated
      if (!authHeaders.Authorization) {
        console.warn('âš ï¸ No authentication token found');
        return {
          success: false,
          documents: [],
          total: 0,
          error: 'Please log in to access documents',
          statusCode: 401
        };
      }
      
      // Add cache buster if force refresh is requested
      const params = {};
      if (forceRefresh) {
        params.cacheBuster = Date.now().toString();
        console.log('ðŸ”„ Force refresh requested with cache buster:', params.cacheBuster);
      }
      
      const response = await axios.get(`${this.apiUrl}/documents`, {
        headers: {
          ...authHeaders,
          'Cache-Control': 'no-cache',
          'Pragma': 'no-cache'
        },
        params: params
      });

      console.log('ðŸ“„ Documents API response:', response.data);
      console.log('ðŸ“„ Response structure:', {
        hasData: !!response.data.data,
        hasDocuments: !!response.data.data?.documents,
        documentsArray: response.data.data?.documents,
        dataKeys: Object.keys(response.data.data || {}),
        totalCount: response.data.data?.totalCount,
        timestamp: response.data.data?.timestamp
      });

      // Handle the response structure from your DocumentController
      const documentsData = response.data.data?.documents || response.data.documents || response.data.data || response.data || [];
      const totalCount = response.data.data?.totalCount || response.data.totalCount || documentsData.length || 0;

      console.log('ðŸ“„ Extracted documents data:', {
        documentsCount: documentsData.length,
        totalCount: totalCount,
        firstDoc: documentsData[0]
      });

      // Transform documents to ensure consistent structure
      const transformedDocuments = Array.isArray(documentsData) ? documentsData.map(doc => ({
        id: doc.id || doc.documentGuid,
        documentGuid: doc.documentGuid || doc.document_guid,
        fileName: doc.fileName || doc.file_name,
        category: doc.documentCategory || doc.document_category || doc.category || 'Uncategorized',
        size: doc.fileSize || doc.file_size || 'Unknown',
        uploadDate: doc.uploadTime || doc.upload_time || doc.date_published || new Date().toISOString(),
        isActive: doc.isActive !== undefined ? doc.isActive : doc.is_active,
        uploadStatus: doc.uploadStatus || doc.upload_status || (doc.isActive || doc.is_active ? 'active' : 'inactive'),
        numberOfPages: doc.numberOfPages || doc.number_of_pages,
        fileType: doc.fileType || doc.file_type,
        rawContent: doc.rawContent || doc.raw_content
      })) : [];

      console.log('âœ… Transformed documents:', transformedDocuments.length, 'documents found');

      return {
        success: true,
        documents: transformedDocuments,
        total: totalCount,
        timestamp: response.data.data?.timestamp,
        cacheBuster: response.data.data?.cacheBuster
      };
    } catch (error) {
      console.error('âŒ Error fetching documents:', error);
      console.error('Error details:', {
        status: error.response?.status,
        statusText: error.response?.statusText,
        data: error.response?.data,
        apiUrl: `${this.apiUrl}/documents`,
        headers: authHeaders
      });
      
      let errorMessage = error.message;
      
      if (error.response?.status === 401) {
        errorMessage = 'Authentication required. Please make sure you are logged in.';
      } else if (error.response?.status === 403) {
        errorMessage = 'Access denied. Please check your permissions.';
      } else if (error.response?.status === 404) {
        errorMessage = 'Documents API not found. Please check server configuration.';
      } else if (error.response?.status >= 500) {
        errorMessage = 'Server error. Please try again later.';
      }
      
      // Return empty array instead of throwing error to prevent UI crashes
      return {
        success: false,
        documents: [],
        total: 0,
        error: errorMessage,
        statusCode: error.response?.status
      };
    }
  }

  /**
   * Force refresh documents list with cache busting
   */
  async forceRefreshDocuments() {
    console.log('ðŸ”„ Force refreshing documents list...');
    return await this.fetchDocuments(true);
  }

  /**
   * Get document content by ID
   */
  async getDocumentContent(documentId) {
    try {
      const authHeaders = await getAuthHeaders();
      console.log(`ðŸ” Fetching content for document: ${documentId}`);
      
      // Try to get document content from the backend
      const response = await axios.get(`${this.apiUrl}/documents/${documentId}/content`, {
        headers: authHeaders
      });

      console.log('ðŸ“„ Document content response:', response.data);

      // Handle different response structures
      const content = response.data.content || response.data.data?.content || response.data.rawContent || response.data.data;
      const metadata = response.data.metadata || response.data.data?.metadata || {};

      return {
        success: true,
        content: content,
        metadata: metadata
      };
    } catch (error) {
      console.error('âŒ Error fetching document content:', error);
      
      // If the document has rawContent already, use that
      try {
        const documents = await this.fetchDocuments();
        const document = documents.documents.find(doc => doc.id === documentId || doc.documentGuid === documentId);
        
        if (document && document.rawContent) {
          console.log('âœ… Using rawContent from document metadata');
          return {
            success: true,
            content: document.rawContent,
            metadata: {
              fileName: document.fileName,
              category: document.category,
              source: 'rawContent'
            }
          };
        }
      } catch (fallbackError) {
        console.warn('Fallback to rawContent failed:', fallbackError);
      }
      
      throw new Error(`Failed to fetch document content: ${error.message}`);
    }
  }

  /**
   * Get document content from Azure Blob Storage
   */
  async getDocumentFromBlobStorage(documentId) {
    try {
      // This would need the blob URL or container information
      // For now, we'll use a placeholder implementation
      const blobUrl = `${this.blobStorageUrl}/documents/${documentId}`;
      
      const response = await axios.get(blobUrl, {
        responseType: 'text'
      });

      return {
        success: true,
        content: response.data,
        metadata: {
          source: 'blob-storage',
          fetchedAt: new Date().toISOString()
        }
      };
    } catch (error) {
      throw new Error(`Failed to fetch from blob storage: ${error.message}`);
    }
  }

  /**
   * Extract text content from different file types
   */
  async extractTextContent(file) {
    try {
      const formData = new FormData();
      formData.append('file', file);

      const authHeaders = await getAuthHeaders();
      const response = await axios.post(`${this.apiUrl}/documents/extract-text`, formData, {
        headers: {
          ...authHeaders,
          'Content-Type': 'multipart/form-data'
        }
      });

      return {
        success: true,
        content: response.data.content,
        metadata: response.data.metadata
      };
    } catch (error) {
      console.error('Error extracting text content:', error);
      throw new Error(`Failed to extract text: ${error.message}`);
    }
  }

  /**
   * Search documents by query
   */
  async searchDocuments(query, filters = {}) {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.post(`${this.apiUrl}/documents/search`, {
        query,
        filters,
        includeContent: true
      }, {
        headers: authHeaders
      });

      return {
        success: true,
        documents: response.data.documents || [],
        total: response.data.total || 0,
        query
      };
    } catch (error) {
      console.error('Error searching documents:', error);
      throw new Error(`Failed to search documents: ${error.message}`);
    }
  }

  /**
   * Get document categories
   */
  async getDocumentCategories() {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.get(`${this.apiUrl}/document-categories`, {
        headers: authHeaders
      });

      return {
        success: true,
        categories: response.data.categories || response.data || []
      };
    } catch (error) {
      console.error('Error fetching categories:', error);
      return {
        success: false,
        categories: ['General', 'Legal', 'Financial', 'Technical', 'HR']
      };
    }
  }

  /**
   * Process document with intelligent chunking for AI analysis
   * Uses backend LangChain processing for optimal content extraction
   */
  async processDocumentForAI(documentId, chunkingStrategy = 'balanced') {
    try {
      const authHeaders = await getAuthHeaders();
      
      console.log(`ðŸ”„ Processing document ${documentId} with chunking strategy: ${chunkingStrategy}`);
      
      const response = await axios.post(
        `${this.apiUrl}/documents/${documentId}/process-for-ai`,
        {
          chunkingStrategy,
          maxTokensPerChunk: 2000,
          overlapTokens: 200
        },
        {
          headers: authHeaders,
          timeout: 60000 // 60 second timeout for processing
        }
      );

      if (response.data.success) {
        console.log(`âœ… Document processed successfully:`, {
          chunks: response.data.processingInfo?.totalChunks,
          strategy: response.data.processingInfo?.strategy
        });
        
        return {
          success: true,
          documentId,
          originalContent: response.data.originalContent,
          chunks: response.data.chunks,
          processingInfo: response.data.processingInfo,
          metadata: {
            processedAt: new Date().toISOString(),
            chunkingStrategy
          }
        };
      } else {
        throw new Error(response.data.error || 'Document processing failed');
      }
    } catch (error) {
      console.error('âŒ Error processing document for AI:', error);
      
      // Fallback to legacy method if new processing fails
      console.log('ðŸ”„ Falling back to legacy document preparation...');
      try {
        const legacyResult = await this.prepareDocumentForAI(documentId);
        
        // Convert legacy result to chunked format
        const content = legacyResult.document?.content || '';
        const chunks = content.length > 8000 ? [
          {
            content: content.substring(0, 8000),
            metadata: {
              chunkIndex: 0,
              chunkId: `${documentId}_legacy_chunk_0`,
              isFirstChunk: true,
              isLastChunk: content.length <= 8000,
              totalChunks: content.length > 8000 ? 2 : 1,
              fallbackMethod: true
            }
          },
          ...(content.length > 8000 ? [{
            content: content.substring(8000, 16000),
            metadata: {
              chunkIndex: 1,
              chunkId: `${documentId}_legacy_chunk_1`,
              isFirstChunk: false,
              isLastChunk: true,
              totalChunks: 2,
              fallbackMethod: true
            }
          }] : [])
        ] : [{
          content: content,
          metadata: {
            chunkIndex: 0,
            chunkId: `${documentId}_legacy_chunk_0`,
            isFirstChunk: true,
            isLastChunk: true,
            totalChunks: 1,
            fallbackMethod: true
          }
        }];

        return {
          success: true,
          documentId,
          originalContent: {
            text: content,
            metadata: legacyResult.document?.metadata || {}
          },
          chunks: chunks,
          processingInfo: {
            totalChunks: chunks.length,
            strategy: 'legacy_fallback',
            note: 'Using fallback method due to processing service unavailability'
          }
        };
      } catch (fallbackError) {
        return {
          success: false,
          error: `Processing failed and fallback failed: ${error.message}`,
          documentId
        };
      }
    }
  }

  /**
   * Prepare document for AI processing (Legacy method)
   */
  async prepareDocumentForAI(documentId) {
    try {
      console.log(`ðŸ” Preparing document for AI: ${documentId}`);
      
      // First, get all documents to find the specific one
      const documentsResult = await this.fetchDocuments();
      const document = documentsResult.documents.find(doc => 
        doc.id === documentId || doc.documentGuid === documentId
      );

      if (!document) {
        throw new Error(`Document with ID ${documentId} not found`);
      }

      console.log('ðŸ“„ Found document:', document.fileName);

      // Get document content
      const contentResult = await this.getDocumentContent(documentId);

      if (!contentResult.success) {
        throw new Error('Failed to get document content');
      }

      const preparedDocument = {
        id: document.id || document.documentGuid,
        fileName: document.fileName,
        category: document.category,
        size: document.size,
        uploadDate: document.uploadDate,
        content: contentResult.content,
        metadata: {
          ...document,
          ...contentResult.metadata,
          preparedAt: new Date().toISOString()
        }
      };

      console.log('âœ… Document prepared for AI processing:', {
        fileName: preparedDocument.fileName,
        contentLength: preparedDocument.content?.length || 0,
        category: preparedDocument.category
      });

      return {
        success: true,
        document: preparedDocument
      };
    } catch (error) {
      console.error('âŒ Error preparing document for AI:', error);
      throw new Error(`Failed to prepare document: ${error.message}`);
    }
  }

  /**
   * Prepare multiple documents for batch processing
   */
  async prepareDocumentsForBatch(documentIds) {
    const documents = [];
    const errors = [];

    for (const docId of documentIds) {
      try {
        const result = await this.prepareDocumentForAI(docId);
        if (result.success) {
          documents.push(result.document);
        }
      } catch (error) {
        errors.push({
          documentId: docId,
          error: error.message
        });
      }
    }

    return {
      success: documents.length > 0,
      documents,
      errors,
      total: documentIds.length,
      processed: documents.length,
      failed: errors.length
    };
  }

  /**
   * Save workflow execution results
   */
  async saveWorkflowResults(workflowId, results, metadata = {}) {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.post(`${this.apiUrl}/ai-workflow/results`, {
        workflowId,
        results,
        metadata: {
          ...metadata,
          savedAt: new Date().toISOString()
        }
      }, {
        headers: authHeaders
      });

      return {
        success: true,
        resultId: response.data.id,
        message: 'Results saved successfully'
      };
    } catch (error) {
      console.error('Error saving workflow results:', error);
      throw new Error(`Failed to save results: ${error.message}`);
    }
  }

  /**
   * Get workflow execution history
   */
  async getWorkflowHistory(workflowId) {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.get(`${this.apiUrl}/ai-workflow/${workflowId}/history`, {
        headers: authHeaders
      });

      return {
        success: true,
        history: response.data.history || [],
        total: response.data.total || 0
      };
    } catch (error) {
      console.error('Error fetching workflow history:', error);
      return {
        success: false,
        history: [],
        total: 0
      };
    }
  }

  /**
   * Upload and process new document
   */
  async uploadAndProcess(file, category, workflowConfig = null) {
    try {
      // Upload document first
      const formData = new FormData();
      formData.append('pdfFile', file);
      formData.append('category', category);

      const authHeaders = await getAuthHeaders();
      const uploadResponse = await axios.post(`${this.apiUrl}/upload/pdf`, formData, {
        headers: {
          ...authHeaders,
          'Content-Type': 'multipart/form-data'
        }
      });

      const uploadedDocument = uploadResponse.data.data;

      // Force refresh documents list to include the newly uploaded document
      console.log('ðŸ”„ Refreshing documents list after upload...');
      try {
        await this.forceRefreshDocuments();
        console.log('âœ… Documents list refreshed successfully');
      } catch (refreshError) {
        console.warn('âš ï¸ Failed to refresh documents list:', refreshError.message);
        // Don't fail the upload if refresh fails
      }

      // If workflow config is provided, process immediately
      if (workflowConfig) {
        // Extract text content
        const textResult = await this.extractTextContent(file);
        
        return {
          success: true,
          document: uploadedDocument,
          content: textResult.content,
          readyForProcessing: true
        };
      }

      return {
        success: true,
        document: uploadedDocument,
        readyForProcessing: false
      };
    } catch (error) {
      console.error('Error uploading and processing document:', error);
      throw new Error(`Failed to upload document: ${error.message}`);
    }
  }

  /**
   * Get document statistics for workflow dashboard
   */
  async getDocumentStats() {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.get(`${this.apiUrl}/documents/stats`, {
        headers: authHeaders
      });

      return {
        success: true,
        stats: response.data
      };
    } catch (error) {
      console.error('Error fetching document stats:', error);
      // Return mock stats as fallback
      return {
        success: false,
        stats: {
          totalDocuments: 0,
          categories: {},
          recentUploads: 0,
          totalSize: '0 MB'
        }
      };
    }
  }

  /**
   * Check if document is ready for AI processing
   */
  async checkDocumentReadiness(documentId) {
    try {
      const authHeaders = await getAuthHeaders();
      const response = await axios.get(`${this.apiUrl}/documents/${documentId}/readiness`, {
        headers: authHeaders
      });

      return {
        success: true,
        ready: response.data.ready,
        status: response.data.status,
        issues: response.data.issues || []
      };
    } catch (error) {
      // Assume document is ready if check fails
      return {
        success: true,
        ready: true,
        status: 'ready',
        issues: []
      };
    }
  }

  /**
   * Get supported file types for upload
   */
  getSupportedFileTypes() {
    return [
      { ext: 'pdf', type: 'application/pdf', name: 'PDF Documents' },
      { ext: 'docx', type: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', name: 'Word Documents' },
      { ext: 'txt', type: 'text/plain', name: 'Text Files' },
      { ext: 'md', type: 'text/markdown', name: 'Markdown Files' },
      { ext: 'csv', type: 'text/csv', name: 'CSV Files' },
      { ext: 'json', type: 'application/json', name: 'JSON Files' }
    ];
  }

  /**
   * Validate file for upload
   */
  validateFile(file) {
    const supportedTypes = this.getSupportedFileTypes();
    const maxSize = 50 * 1024 * 1024; // 50MB

    const issues = [];

    if (!file) {
      issues.push('No file selected');
      return { valid: false, issues };
    }

    if (file.size > maxSize) {
      issues.push(`File size exceeds 50MB limit (${(file.size / 1024 / 1024).toFixed(2)}MB)`);
    }

    const isSupported = supportedTypes.some(type => 
      file.type === type.type || file.name.toLowerCase().endsWith(`.${type.ext}`)
    );

    if (!isSupported) {
      issues.push(`File type not supported. Supported types: ${supportedTypes.map(t => t.ext).join(', ')}`);
    }

    return {
      valid: issues.length === 0,
      issues,
      fileInfo: {
        name: file.name,
        size: file.size,
        type: file.type,
        sizeFormatted: `${(file.size / 1024 / 1024).toFixed(2)}MB`
      }
    };
  }

  /**
   * Test blob chunks workflow execution
   * Call this from browser console: window.testBlobChunksWorkflow()
   */
  async testBlobChunksWorkflow() {
    console.log('ðŸš€ Testing Blob Chunks Workflow Execution\n');
    
    try {
      // Import the workflow execution service
      const { workflowExecutionService } = await import('./workflowExecutionService');
      
      // Create test workflow
      const nodes = [
        {
          id: 'trigger-1',
          type: 'trigger',
          data: {
            label: 'Document Upload',
            config: {
              triggerType: 'documentUpload',
              selectedDocuments: ['38d492da-a38c-468a-aaca-118760d099d6']
            }
          }
        },
        {
          id: 'ai-1',
          type: 'aiAgent',
          data: {
            label: 'GPT-4o Mini Analyzer',
            config: {
              modelType: 'gpt4o-mini',
              systemPrompt: 'You are a helpful AI assistant that analyzes documents.',
              userPrompt: 'Please analyze this document and provide key insights: {DOCUMENT_CONTENT}',
              temperature: 0.7,
              maxTokens: 1000
            }
          }
        }
      ];
      
      const edges = [{ id: 'e1', source: 'trigger-1', target: 'ai-1' }];
      
      console.log('ðŸ“Š Executing workflow with blob chunks integration...');
      const result = await workflowExecutionService.executeWorkflow(nodes, edges);
      
      console.log('\nðŸŽ‰ Workflow Results:');
      console.log('='.repeat(60));
      console.log(`âœ… Success: ${result.success}`);
      console.log(`ðŸ“ Summary: ${result.summary}`);
      console.log(`ðŸ“Š Nodes: ${result.results.length}`);
      
      result.results.forEach((nodeResult, index) => {
        console.log(`\nðŸ“‹ Node ${index + 1}: ${nodeResult.nodeName}`);
        console.log(`   Status: ${nodeResult.status}`);
        console.log(`   Duration: ${nodeResult.duration}ms`);
        console.log(`   Summary: ${nodeResult.summary}`);
        
        if (nodeResult.data?.documentChunks) {
          const docs = Object.keys(nodeResult.data.documentChunks);
          console.log(`   ðŸ“„ Documents: ${docs.length}`);
          docs.forEach(docId => {
            const doc = nodeResult.data.documentChunks[docId];
            console.log(`      - ${docId}: ${doc.content.length} chars`);
            if (doc.processingInfo) {
              console.log(`        Chunks: ${doc.processingInfo.selectedChunks}/${doc.processingInfo.totalChunks}`);
              console.log(`        Source: ${doc.metadata.blobSource ? 'blob_storage' : 'fallback'}`);
            }
          });
        }
        
        if (nodeResult.data?.aiResults) {
          console.log(`   ðŸ¤– AI Results: ${nodeResult.data.successful}/${nodeResult.data.processed}`);
          nodeResult.data.aiResults.forEach(aiResult => {
            if (aiResult.success) {
              console.log(`      âœ… ${aiResult.fileName}: Analysis completed`);
              console.log(`         Model: ${aiResult.model}, Time: ${aiResult.processingTime}ms`);
              console.log(`         Preview: "${aiResult.analysis?.substring(0, 100) || 'No analysis'}..."`);
            } else {
              console.log(`      âŒ ${aiResult.documentId}: ${aiResult.error}`);
            }
          });
        }
      });
      
      console.log('\n' + '='.repeat(60));
      console.log('ðŸŽ¯ Blob Chunks Integration Verified:');
      console.log('   âœ… Documents from blob storage (not encrypted content)');
      console.log('   âœ… Clean text processing');
      console.log('   âœ… Token-efficient AI analysis');
      console.log('   âœ… Proper workflow execution');
      
      return result;
      
    } catch (error) {
      console.error('âŒ Test failed:', error);
      throw error;
    }
  }
}

// Create and export singleton instance
const workflowDocumentService = new WorkflowDocumentService();

// Make test function available globally
if (typeof window !== 'undefined') {
  window.testBlobChunksWorkflow = () => workflowDocumentService.testBlobChunksWorkflow();
  console.log('ðŸ’¡ Test function available: window.testBlobChunksWorkflow()');
}

export default workflowDocumentService;
